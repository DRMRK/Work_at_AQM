{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is implementation of logistic regression with regularization\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#Import Libraries to do thing the blackbox way \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'spambase.csv' does not exist: b'spambase.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-7da7ae2c152b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Load the data, this dataset is for spam classification from UCI ML repository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spambase.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Applications/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'spambase.csv' does not exist: b'spambase.csv'"
     ]
    }
   ],
   "source": [
    "#Load the data, this dataset is for spam classification from UCI ML repository\n",
    "df=pd.read_csv('spambase.csv',header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.104553     0.213015     0.280656     0.065425     0.312223   \n",
       "std       0.305358     1.290575     0.504143     1.395151     0.672513   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.420000     0.000000     0.380000   \n",
       "max       4.540000    14.280000     5.100000    42.810000    10.000000   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.095901     0.114208     0.105295     0.090067     0.239413   \n",
       "std       0.273824     0.391441     0.401071     0.278616     0.644755   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     0.000000     0.000000     0.160000   \n",
       "max       5.880000     7.270000    11.110000     5.260000    18.180000   \n",
       "\n",
       "          ...                48           49           50           51  \\\n",
       "count     ...       4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      ...          0.038575     0.139030     0.016976     0.269071   \n",
       "std       ...          0.243471     0.270355     0.109394     0.815672   \n",
       "min       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "25%       ...          0.000000     0.000000     0.000000     0.000000   \n",
       "50%       ...          0.000000     0.065000     0.000000     0.000000   \n",
       "75%       ...          0.000000     0.188000     0.000000     0.315000   \n",
       "max       ...          4.385000     9.752000     4.081000    32.478000   \n",
       "\n",
       "                52           53           54           55            56  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   4601.000000   \n",
       "mean      0.075811     0.044238     5.191515    52.172789    283.289285   \n",
       "std       0.245882     0.429342    31.729449   194.891310    606.347851   \n",
       "min       0.000000     0.000000     1.000000     1.000000      1.000000   \n",
       "25%       0.000000     0.000000     1.588000     6.000000     35.000000   \n",
       "50%       0.000000     0.000000     2.276000    15.000000     95.000000   \n",
       "75%       0.052000     0.000000     3.706000    43.000000    266.000000   \n",
       "max       6.003000    19.829000  1102.500000  9989.000000  15841.000000   \n",
       "\n",
       "                57  \n",
       "count  4601.000000  \n",
       "mean      0.394045  \n",
       "std       0.488698  \n",
       "min       0.000000  \n",
       "25%       0.000000  \n",
       "50%       0.000000  \n",
       "75%       1.000000  \n",
       "max       1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last row is target variable, separate it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X,y to numpy arrays\n",
    "x = df.iloc[:,0:57]\n",
    "d = df.iloc[:,57]\n",
    "N=x.shape[1]\n",
    "#y is in the form of a series, make it an array\n",
    "y=np.asmatrix(d)\n",
    "#Need to take transpose\n",
    "y=y.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Training Set and Testing Set\n",
    "x_train1, x_test, y_train, y_test = train_test_split(x,y,test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/franciumpnc/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/Users/franciumpnc/anaconda3/lib/python3.7/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/Users/franciumpnc/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Since the feature values are over a wide range do Feature Scaling\n",
    "sc_X=StandardScaler()\n",
    "x_train1=sc_X.fit_transform(x_train1)\n",
    "x_test=sc_X.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Insert 1 s in the x s, this is the bias term in logistic regression\n",
    "x_train=np.insert(x_train1,0,values=1,axis=1)\n",
    "x_test=np.insert(x_test,0,values=1,axis=1)\n",
    "N=x_train.shape[1]\n",
    "beta=np.random.uniform(low=.001,high=.1,size=(N,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2300, 58)\n",
      "(58, 1)\n"
     ]
    }
   ],
   "source": [
    "#Print the shapes and see if it makes sense\n",
    "print(x_train.shape)\n",
    "print(beta.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the sigmoid function\n",
    "def sigmoid(x,beta):\n",
    "    #x is your feature and beta is the parameters\n",
    "    return (1/(1+np.exp(-(x@beta))))\n",
    "\n",
    "#Logistic Regression class. \n",
    "class grad_descent:\n",
    "    #This takes, target_train, feature_train, beta,target_validation, feature_validation\n",
    "    # regularization parameter, learning rate, iteration number\n",
    "    def __init__(self,y_t,x_t,beta,y_v,x_v,lmda,learning_rate,max_iter):\n",
    "        self.y_t=y_t\n",
    "        self.x_t=x_t\n",
    "        self.beta=beta\n",
    "        self.y_v=y_v\n",
    "        self.x_v=x_v\n",
    "        self.lmda=lmda\n",
    "        self.learning_rate=learning_rate\n",
    "        self.max_iter=max_iter\n",
    "    \n",
    "    #The sigmoid function\n",
    "    def sigmoid(self):\n",
    "        return (1/(1+np.exp(-(self.x_t@self.beta)))) \n",
    "   \n",
    "    #The gradient   \n",
    "    def gradient(self):\n",
    "        return (1/len(self.x_t))*(self.x_t.T@(sigmoid(self.x_t,self.beta)-self.y_t)+2*self.lmda*self.beta)    \n",
    "\n",
    "    #Define Loglikelihood\n",
    "    def loglikelihood(self,x,y):\n",
    "    #Ridge regression, excluding the beta_0 term from regularization\n",
    "        reg=self.lmda*self.beta[1:len(self.beta)].T@self.beta[1:len(self.beta)]\n",
    "        return -(1/len(x))*(((x@self.beta).T@y-np.ones((1,len(x)))@(np.log(1+np.exp(x@self.beta))))+reg)\n",
    "        #return -(1/len(x))*((x@self.beta).T@y+reg)        \n",
    "    \n",
    "   #Gradient descent \n",
    "    def g_d_cal(self):\n",
    "    #Initialize for training cost\n",
    "        cost_of_train=[]\n",
    "    #Initialize for validation cost\n",
    "        cost_of_valid=[]\n",
    "        it_n=[]\n",
    "        for i in range(self.max_iter):\n",
    "    #Calculate the gradient\n",
    "            dtheta=self.gradient()\n",
    "    #Update theta\n",
    "            self.beta=self.beta-self.learning_rate*dtheta\n",
    "            #The loglikelihood or the cost function\n",
    "    #for training data\n",
    "            cost_train=self.loglikelihood(self.x_t,self.y_t)[0,0]\n",
    "            cost_of_train.append(cost_train)\n",
    "            #for validation data \n",
    "            cost_valid=self.loglikelihood(self.x_v,self.y_v)[0,0]\n",
    "            cost_of_valid.append(cost_valid)\n",
    "            it_n.append(i)\n",
    "        return (self.beta,it_n,cost_of_train,cost_of_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9291304347826087"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initialize beta same length as the number of features, including the bias term\n",
    "N=x_train.shape[1]\n",
    "beta=np.random.uniform(low=-.1,high=.1,size=(N,1))\n",
    "#Do the fitting/minimization to get the values of beta\n",
    "#gd=grad_descent(y_train,x_train,beta,y_test,x_test,0.3,1e-2,10000)\n",
    "gd=grad_descent(y_train,x_train,beta,y_test,x_test,0.5,1e-2,10000)\n",
    "#Return the results from gradient descent,this returns, opt_beta,i,cost for train, cost for validation\n",
    "beta_opt=gd.g_d_cal()\n",
    "plt.plot(beta_opt[1],beta_opt[2],label='train')\n",
    "plt.legend()\n",
    "#plt.legend('test')\n",
    "plt.plot(beta_opt[1],beta_opt[3],label='valid')\n",
    "#plt.legend('train','test')\n",
    "plt.legend()\n",
    "\n",
    "#beta_opt=grad_descent(y_train,x_train,beta,y_test,x_test,0.5,1e-2,1000)\n",
    "from sklearn.metrics import accuracy_score\n",
    "#See how good this is doing\n",
    "y_pred = np.around(sigmoid(x_train,beta_opt[0]))\n",
    "y_true = y_train\n",
    "accuracy_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Randomize the indexes of the dataframe for the k fold validation\n",
    "#This function takes data, number of k fold, regularization parameter,learning rate, \n",
    "#maximum iteration for grad. desc.\n",
    "def cross_validation(data,k_of_fold,lmda,learning_rate,max_iter):\n",
    "    import random\n",
    "#Create an array of indexes\n",
    "    a=np.arange(len(data))\n",
    "#Randomly shuffle the array, this will be used to get the train,validation parts\n",
    "    random.shuffle(a)\n",
    "#How many folds \n",
    "    k_of_fold=5\n",
    "#NUmber of data points in each fold\n",
    "    point_in_fold=int(len(data)/k_of_fold)\n",
    "    ac_sc_v=[]\n",
    "    ac_sc_t=[]\n",
    "    #for each fold test on one part train on the other part\n",
    "    for i in np.arange(k_of_fold):  \n",
    "        #Choose part that will be validation part.\n",
    "        valid=data.iloc[a[int(i*point_in_fold):int(i*point_in_fold+point_in_fold)]]\n",
    "        x_valid= valid.iloc[:,0:57].values\n",
    "    #insert the bias 1 term\n",
    "        x_valid=np.insert(x_valid,0,values=1,axis=1)\n",
    "        y_valid= valid.iloc[:,57]\n",
    "        y_valid=np.asmatrix(y_valid)\n",
    "        y_valid=y_valid.T\n",
    "    ##################\n",
    "        #Delete the validdation part and keep the rest to train\n",
    "        train=data.drop(a[int(i*point_in_fold):int(i*point_in_fold+point_in_fold)])\n",
    "        x_train=train.iloc[:,0:57].values\n",
    "    #insert the bias 1 term\n",
    "        x_train=np.insert(x_train,0,values=1,axis=1)\n",
    "        y_train=train.iloc[:,57]\n",
    "        y_train=np.asmatrix(y_train)\n",
    "        y_train=y_train.T\n",
    "    #Do, feature scalling\n",
    "        sc_X=StandardScaler()\n",
    "        x_train=sc_X.fit_transform(x_train)\n",
    "        x_valid=sc_X.transform(x_valid)\n",
    "   #Initial guess beta\n",
    "        N=x_train.shape[1]\n",
    "        beta=np.random.uniform(low=.001,high=.1,size=(N,1))\n",
    "    #Call gradient descent\n",
    "        gd=grad_descent(y_train,x_train,beta,y_valid,x_valid,lmda,learning_rate,max_iter)\n",
    "        beta_opt=gd.g_d_cal()\n",
    "        #Predict on validation set\n",
    "        y_pred_valid = np.around(sigmoid(x_valid,beta_opt[0]))\n",
    "        a_sc_valid=accuracy_score(y_valid, y_pred_valid)\n",
    "        ac_sc_v.append(a_sc_valid)\n",
    "            #print('append',ac_sc_v)\n",
    "        #Predict on training set\n",
    "        y_pred_train = np.around(sigmoid(x_train,beta_opt[0]))\n",
    "        ac_sc_train=accuracy_score(y_train, y_pred_train)\n",
    "        ac_sc_t.append(ac_sc_train)\n",
    "                #print('append train',ac_sc_t)\n",
    "    #print('lambda=',lmda, 'Avg. accuracy score on training',sum((ac_sc_t))/k_of_fold)\n",
    "    print('lambda=',lmda,'Avg. accuracy score on validation',sum(ac_sc_v)/k_of_fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda= 0.1 Avg. accuracy score on validation 0.9163043478260869\n",
      "lambda= 2 Avg. accuracy score on validation 0.9210869565217392\n",
      "lambda= 3 Avg. accuracy score on validation 0.9180434782608696\n",
      "lambda= 5 Avg. accuracy score on validation 0.921304347826087\n",
      "lambda= 6 Avg. accuracy score on validation 0.9184782608695652\n",
      "lambda= 10 Avg. accuracy score on validation 0.9204347826086956\n"
     ]
    }
   ],
   "source": [
    "#Initialize the regularization parameter\n",
    "lmbda=[0.1,2,3,5,6,10]\n",
    "#Now run the k fold cross validation\n",
    "for i in lmbda:\n",
    "    cross_validation(df,k_of_fold=5,lmda=i,learning_rate=1e-1,max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lambda=5 gives good score on validation. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
